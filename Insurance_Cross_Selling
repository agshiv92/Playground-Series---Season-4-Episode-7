{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-16T06:13:52.965383Z","iopub.execute_input":"2024-07-16T06:13:52.965798Z","iopub.status.idle":"2024-07-16T06:13:53.429901Z","shell.execute_reply.started":"2024-07-16T06:13:52.965763Z","shell.execute_reply":"2024-07-16T06:13:53.428749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 12px; background-color: #ffffff; font-size:130%; text-align:left\">\n<h2 align=\"left\"><font color=#E1B12D>Introduction</font></h2>\nWelcome to the 2024 Kaggle Playground Series, Season 4, Episode 7!\n\nIn this episode, your objective is to predict which customers will respond positively to an automobile insurance offer. This involves creating a binary classification model to accurately identify these responsive customers based on the data provided.\n\n","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#E1B12D>Approach</font></h2>\n\n<a id=\"toc\"></a>\n- [1.1 Import Libraries](#1.1)\n- [1.2 Configuration](#1.2)\n- [1.3 Important Functions](#1.3)\n- [1.4 Import Data](#1.4)\n- [1.5 Quick overview](#1.5)\n- [2. Exploratory Data Analysis ](#2)\n- [3.1 Feature Engineering](#3.1)\n- [3.2 Missing Data Handling](#3.2)\n- [3.3 Outlier Handling](#3.3)\n- [3.4 Feature Transformation](#3.4)\n- [3.5 Feature Creation](#3.5)\n- [3.6 Feature Selection](#3.6)\n- [4. Data Pipeline](#4)\n- [4.1 Pipeline Creation](#4.1)\n- [4.2 Visualizing the Pipeline ](#4.2)\n- [5. Model Building](#5)\n- [5.1 Train Test Split](#5.1)\n- [5.2 Training the Model](#5.2)\n- [5.3 Performance Evaluation](#5.3)\n- [5.4 Ensembling for Performance Improvement](#5.4)\n- [5.5 Model Explainability](#5.5)\n- [6. Model Infrence](#6)\n- [6.1 Loading the Model](#6.1)\n- [6.2 Data Loading and Prepration](#6.2)\n- [6.3 Model Forecasting](#6.3)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n## <b>1.1 <span style='color:#E1B12D'>Import Libraries</span></b> ","metadata":{}},{"cell_type":"code","source":"!pip3 install -q dagshub\n!pip3 install -q mlflow","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-16T06:13:53.432158Z","iopub.execute_input":"2024-07-16T06:13:53.432647Z","iopub.status.idle":"2024-07-16T06:14:41.877477Z","shell.execute_reply.started":"2024-07-16T06:13:53.432611Z","shell.execute_reply":"2024-07-16T06:14:41.876144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# System related tasks\nimport os\nimport gc\nimport warnings\n\n# Data manipulation and numerical operations\nimport numpy as np \nimport pandas as pd  \nimport polars as pl\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px\n\n# Scikit-learn model selection and evaluation\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Machine learning models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n\n# Gradient boosting frameworks\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# Optimization and utilities\nimport optuna\nimport joblib\nfrom kaggle_secrets import UserSecretsClient\n\n# For Feature Selection\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Experiment tracking and collaboration\nimport dagshub\nimport mlflow\n\n# Model Explainability \nimport shap\n\n# Ensure warnings are ignored and plots are displayed inline\nwarnings.filterwarnings('ignore')\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-16T06:14:41.879502Z","iopub.execute_input":"2024-07-16T06:14:41.879970Z","iopub.status.idle":"2024-07-16T06:14:55.131895Z","shell.execute_reply.started":"2024-07-16T06:14:41.879923Z","shell.execute_reply":"2024-07-16T06:14:55.130950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n## <b>1.2 <span style='color:#E1B12D'>Configuration</span></b> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \"\"\"\n    Configuration class for parameters, models, paths, and cross-validation strategies.\n    Please use caps lock for variable names while filling in parameters.\n    \"\"\"\n    # Paths\n    BASE_PATH = \"/kaggle/input/playground-series-s4e7\"\n    TRAIN_PATH = f\"{BASE_PATH}/train.csv\"\n    TEST_PATH = f\"{BASE_PATH}/test.csv\"\n    \n    # Columns Rename to ensure names are readable and relevant\n    COLUMN_RENAME = {\n        'id': 'ID',\n        'Gender': 'Gender', \n        'Age': 'Age', \n        'Driving_License': 'Driving_License', \n        'Region_Code': 'Region_Code', \n        'Previously_Insured': 'Previously_Insured', \n        'Vehicle_Age': 'Vehicle_Age', \n        'Vehicle_Damage': 'Vehicle_Damage', \n        'Annual_Premium': 'Annual_Premium', \n        'Policy_Sales_Channel': 'Policy_Sales_Channel', \n        'Vintage': 'Vintage', \n        'Response': 'Response'\n    }\n    \n    # Data Type Casting Settings to ensure smooth pipeline operation\n    DATA_TYPES = {\n        'ID': 'int64',\n        'Gender': 'category',\n        'Age': 'int64',\n        'Driving_License': 'int64',\n        'Region_Code': 'float64',\n        'Previously_Insured': 'int64',\n        'Vehicle_Age': 'category',\n        'Vehicle_Damage': 'category',\n        'Annual_Premium': 'float64',\n        'Policy_Sales_Channel': 'float64',\n        'Vintage': 'int64',\n        'Response': 'int64'\n    }\n    \n    # Selecting Categorical variables. High-dimensional variables are excluded\n    CATEGORICAL_VARIABLES = ( 'Vehicle_Age','Region_Code','Driving_License','Previously_Insured','Vehicle_Damage_Yes','Gender_Male') \n    \n    # Defining Target Variable \n    TARGET_VARIABLE = 'Response'\n    \n    # Model Settings to train the model with hyperparameters\n    MODELS = {\n        \"xgboost\": {\n            \"model\": xgb.XGBClassifier,\n            \"params\": {\n                'colsample_bylevel': 0.21416299403295808,\n                'colsample_bynode': 0.6208356380953189,\n                'colsample_bytree': 0.19219680400212635,\n                'gamma': 0.6051664515971382,\n                'max_bin': 682,\n                'max_delta_step': 5.264818337431145,\n                'max_depth': 68,\n                'min_child_weight': 5.23408291542125,\n                'n_estimators': 1000,\n                'n_jobs': -1,\n                'objective': \"binary:logistic\",\n                'random_state': 42,\n                'reg_alpha': 0.46516016901463414,\n                'reg_lambda': 0.8410553418334902,\n                'subsample': 0.802533192662325,\n                'verbosity': 0,\n                'eval_metric': \"auc\",\n                \"enable_categorical\": True, \n                'early_stopping_rounds': 10,\n                'tree_method': 'gpu_hist'  # Use GPU acceleration\n            }\n        },\n        \"catboost\": {\n            \"model\": CatBoostClassifier,\n            \"params\": {\n                \"iterations\": 100,\n                \"learning_rate\": 0.05,\n                \"depth\": 9,\n                \"verbose\": False,\n                \"task_type\": \"GPU\",\n                \"devices\": '0:1',\n                \"random_seed\": 42,\n                \"loss_function\": 'Logloss',  # Loss function\n                \"eval_metric\": 'AUC',        # Evaluation metric\n                \"random_strength\": 0,\n                \"l2_leaf_reg\": 0.5\n            }\n        },\n        \"lightgbm\": {\n            \"model\": lgb.LGBMClassifier,\n            \"params\": {\n                \"n_estimators\": 100,\n                \"learning_rate\": 0.05,\n                \"max_depth\": 9,\n                \"subsample\": 1,\n                \"colsample_bytree\": 1,\n                \"random_state\": 42,\n                \"device\": \"gpu\",\n                \"gpu_device_id\": 0,\n                \"max_bin\": 16,\n                \"num_leaves\": 31,\n                \"verbosity\": -1,\n                \"objective\": \"binary\",     # Loss function for binary classification\n                \"metric\": \"auc\"            # Evaluation metric\n            }\n        }\n    }\n    SEED = 42\n    # Cross-Validation Settings for classification problem\n    CV_STRATEGY = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n    \n    # MLflow Settings to track the performance of model training\n    MLFLOW_TRACKING_URI = \"https://dagshub.com/agshiv92/GrupoBimbo_InventoryDemand_Solutions.mlflow\"\n    MLFLOW_EXPERIMENT_NAME = \"Group_Bimbo_Predictive_Model\"\n    DAGSHUB_REPO_OWNER = \"agshiv92\"\n    DAGSHUB_REPO = \"GrupoBimbo_InventoryDemand_Solutions\"\n    MLFLOW_URI = \"https://dagshub.com/username/repository_name.mlflow\"\n    MLFLOW_LOCAL_URI = \"/kaggle/working/mlflow\"\n    \n    # Setting seed to ensure reproducibility\n    \n    \n    # Model Save Path \n    MODEL_SAVE_PATH = '/kaggle/working/models'\n    EXPERIMENT_RECORDING = 'LOCAL'\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:37:57.503599Z","iopub.execute_input":"2024-07-16T06:37:57.503986Z","iopub.status.idle":"2024-07-16T06:37:57.520154Z","shell.execute_reply.started":"2024-07-16T06:37:57.503955Z","shell.execute_reply":"2024-07-16T06:37:57.519039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.3\"></a>\n## <b>1.3 <span style='color:#E1B12D'>Important Functions </span></b> ","metadata":{}},{"cell_type":"code","source":"# Defining some important function which will be used for the analysis of data\ndef inspect_columns(df, df_name):\n    \"\"\"A helper function that does a better job than df.info() and df.describe()\"\"\"\n    \n    total_rows = len(df)\n    result = pd.DataFrame({\n        'total_rows': [total_rows] * df.shape[1],\n        'rows_with_missing_values': df.isnull().sum(),\n        'unique': df.nunique() == total_rows,\n        'cardinality': df.nunique(),\n        'with_null': df.isna().any(),\n        'null_pct': round((df.isnull().sum() / total_rows) * 100, 2),\n        '1st_row': df.iloc[0],\n        'random_row': df.iloc[np.random.randint(low=0, high=total_rows)],\n        'last_row': df.iloc[-1],\n        'dtype': df.dtypes,\n    })\n    \n    # Print the name of the dataframe\n    print(f\"\\n{'='*10} {df_name} {'='*10}\\n\")\n    \n    # Print the head of the dataframe\n    print(\"First few rows of the dataframe:\\n\")\n    display(df.head())\n    \n    # Print the resulting statistics\n    print(\"Detailed statistics:\\n\")\n    display(result)\n    \ndef cast_data_types(df, data_types):\n    \"\"\"A helper function to cast the pre determined data type for the columns in dataframe\"\"\"\n    for column,data_type in data_types.items():\n        if column in df.columns:\n            df[column] = df[column].astype(data_type)\n    return df\n\ndef get_or_create_experiment_id(name):\n    \"\"\"Function to create or get the experiment id\"\"\"\n    exp = mlflow.get_experiment_by_name(name)\n    if exp is None:\n        exp_id = mlflow.create_experiment(name)\n        return exp_id\n    return exp.experiment_id\n\ndef reduce_memory_usage(df):\n    \"\"\"Reduce memory usage of a dataframe by downcasting data types.\"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n#     print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        # Skip timestamp columns\n        if pd.api.types.is_datetime64_any_dtype(df[col]):\n            continue\n        if (col_type != object) & (col_type != object):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n#     print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n    return df\n\ndef check_and_create_directory(directory_path):\n    \"\"\" To create directory if not exist\"\"\"\n    if not os.path.exists(directory_path):\n        os.makedirs(directory_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:33:28.076361Z","iopub.execute_input":"2024-07-16T06:33:28.077081Z","iopub.status.idle":"2024-07-16T06:33:28.100679Z","shell.execute_reply.started":"2024-07-16T06:33:28.077046Z","shell.execute_reply":"2024-07-16T06:33:28.099412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.4\"></a>\n## <b>1.4 <span style='color:#E1B12D'>Import Data</span></b> ","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv(CFG.TRAIN_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:33:28.570662Z","iopub.execute_input":"2024-07-16T06:33:28.571677Z","iopub.status.idle":"2024-07-16T06:33:45.298792Z","shell.execute_reply.started":"2024-07-16T06:33:28.571637Z","shell.execute_reply":"2024-07-16T06:33:45.297695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:33:45.301059Z","iopub.execute_input":"2024-07-16T06:33:45.301724Z","iopub.status.idle":"2024-07-16T06:33:45.325390Z","shell.execute_reply.started":"2024-07-16T06:33:45.301679Z","shell.execute_reply":"2024-07-16T06:33:45.323915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.5\"></a>\n## <b>1.5 <span style='color:#E1B12D'>Quick overview</span></b> ","metadata":{}},{"cell_type":"code","source":"inspect_columns(train_df,\"Training Dataframe\")","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:33:45.327568Z","iopub.execute_input":"2024-07-16T06:33:45.328070Z","iopub.status.idle":"2024-07-16T06:34:05.109534Z","shell.execute_reply.started":"2024-07-16T06:33:45.328017Z","shell.execute_reply":"2024-07-16T06:34:05.108391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### About Data:\n    \nThe dataset provided contains information related to vehicle insurance. Here is a detailed description of the columns:\n\n#### Columns Description\n\n- **id**: Unique identifier for each record.\n- **Gender**: Gender of the insured individual.\n- **Age**: Age of the insured individual.\n- **Driving_License**: Indicator if the individual holds a driving license (1: Yes, 0: No).\n- **Region_Code**: Unique code for the region of the insured individual.\n- **Previously_Insured**: Indicator if the individual was previously insured (1: Yes, 0: No).\n- **Vehicle_Age**: Age of the vehicle ('< 1 Year', '1-2 Year', '> 2 Years').\n- **Vehicle_Damage**: Indicator if the vehicle was damaged previously (Yes/No).\n- **Annual_Premium**: The annual premium amount for the insurance.\n- **Policy_Sales_Channel**: Sales channel through which the policy was sold.\n- **Vintage**: Number of days the customer has been associated with the insurance company.\n- **Response**: Response from the customer (1: Interested, 0: Not interested).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #E1B12D; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n<a id=\"2\"></a>\n## <b>2 <span style='color:#E1B12D'> Exploratory Data Analysis</span></b> ","metadata":{}},{"cell_type":"markdown","source":"TBC","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n## <b>3 <span style='color:#E1B12D'> Feature Engineering </span></b> \n### <b>3.1 <span style='color:#E1B12D'> Missing Data Handling </span></b> ","metadata":{}},{"cell_type":"markdown","source":"There is no missing data in the dataframe. If there are missing data there are there are below methods to handle missing data. \n\n### Handling Missing Data\n\n1. **Remove Missing Data**\n   - Remove rows with missing values\n   - Remove columns with missing values\n\n2. **Impute Missing Data**\n   - **Mean Imputation**: Replace missing values with the mean.\n   - **Median Imputation**: Replace missing values with the median.\n   - **Mode Imputation**: Replace missing values with the mode.\n   - **Forward Fill**: Replace missing values with the previous value.\n   - **Backward Fill**: Replace missing values with the next value.\n   - **Interpolation**: Use linear interpolation to estimate missing values.\n   - **K-Nearest Neighbors (KNN) Imputation**: Use the nearest neighbors to impute missing values.\n   - **Predictive Imputation**: Use regression or other models to predict missing values.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n### <b>3.2 <span style='color:#E1B12D'> Outlier Handling </span></b> ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n### <b>3.3 <span style='color:#E1B12D'> Feature Transformation </span></b> ","metadata":{}},{"cell_type":"markdown","source":"#### 3.3.0 Data Prepration\nData prepration by giving appropriate column name and converting the data in appropriate data type","metadata":{}},{"cell_type":"code","source":"def data_prepration(df):\n    df = reduce_memory_usage(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:05.112163Z","iopub.execute_input":"2024-07-16T06:34:05.112516Z","iopub.status.idle":"2024-07-16T06:34:05.117567Z","shell.execute_reply.started":"2024-07-16T06:34:05.112486Z","shell.execute_reply":"2024-07-16T06:34:05.116335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.1 One Hot Encoding\n\nI have chosen to avoid one-hot encoding for categorical variables. Instead, I will be leveraging advanced machine learning algorithms like LightGBM, which natively support categorical data. This decision is driven by three key factors:- Efficiency, Performance and \nSimplicity","metadata":{}},{"cell_type":"code","source":"def one_hot_encode_train(train_df, categorical_columns):\n    encoder = OneHotEncoder(drop='first', sparse=False)\n    encoded_df = encoder.fit_transform(train_df[categorical_columns])\n    encoded_df = pd.DataFrame(encoded_df, columns=encoder.get_feature_names_out(categorical_columns))\n    train_df = train_df.drop(columns=categorical_columns).reset_index(drop=True)\n    train_df = pd.concat([train_df, encoded_df], axis=1)\n    joblib.dump(encoder, 'encoder.pkl')  # Save the fitted encoder to a file\n    return train_df","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:05.119195Z","iopub.execute_input":"2024-07-16T06:34:05.119557Z","iopub.status.idle":"2024-07-16T06:34:05.131611Z","shell.execute_reply.started":"2024-07-16T06:34:05.119526Z","shell.execute_reply":"2024-07-16T06:34:05.130346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_vehicle_age(df):\n    vehicle_age_mapping = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n    df['Vehicle_Age'] = df['Vehicle_Age'].map(vehicle_age_mapping)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:05.133215Z","iopub.execute_input":"2024-07-16T06:34:05.133586Z","iopub.status.idle":"2024-07-16T06:34:05.146292Z","shell.execute_reply.started":"2024-07-16T06:34:05.133556Z","shell.execute_reply":"2024-07-16T06:34:05.145263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.2 Standard Scaler\nLagged features are the demand in previouse week for the same product and client. This feature is helpful to understand the dempendency of the demand on previous history","metadata":{}},{"cell_type":"code","source":"def standard_scale_train(train_df, numerical_columns):\n    scaler = StandardScaler()\n    train_df[numerical_columns] = scaler.fit_transform(train_df[numerical_columns])\n    joblib.dump(scaler, 'scaler.pkl')  # Save the fitted scaler to a file\n    return train_df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:05.147612Z","iopub.execute_input":"2024-07-16T06:34:05.147985Z","iopub.status.idle":"2024-07-16T06:34:05.160011Z","shell.execute_reply.started":"2024-07-16T06:34:05.147952Z","shell.execute_reply":"2024-07-16T06:34:05.158880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.3 Rolling Features\nRolling features capture the aggregated values over a specific window of time, such as the average or sum of demand over the past few weeks. These features are instrumental in understanding trends and patterns in the data.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n### <b>3.4 <span style='color:#E1B12D'> Feature Creation </span></b> \n\n#### 3.4.1 Sales Growth","metadata":{}},{"cell_type":"code","source":"def add_interaction_features(df, col1, col2):\n    interaction_feature_name = f\"{col1}_{col2}\"\n    df[interaction_feature_name] = df[col1] * df[col2]\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:05.161544Z","iopub.execute_input":"2024-07-16T06:34:05.162016Z","iopub.status.idle":"2024-07-16T06:34:05.172084Z","shell.execute_reply.started":"2024-07-16T06:34:05.161975Z","shell.execute_reply":"2024-07-16T06:34:05.170823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_cols = ['Age', 'Annual_Premium', 'Vintage']\ncategorical_cols = ['Gender', 'Vehicle_Damage']\ntrain_df = data_prepration(train_df)\ntrain_df = one_hot_encode_train(train_df, categorical_cols)\ntrain_df = map_vehicle_age(train_df)\ntrain_df = standard_scale_train(train_df, numerical_cols)\ntrain_df = add_interaction_features(train_df, 'Age', 'Annual_Premium')\ntrain_df = add_interaction_features(train_df, 'Age', 'Vintage')\ntrain_df['Region_Code'] = train_df['Region_Code'].astype('int')\ntrain_df['Region_Code'] = train_df['Region_Code'].astype('category')\ntrain_df['Driving_License'] = train_df['Driving_License'].astype('category')\ntrain_df['Previously_Insured'] = train_df['Previously_Insured'].astype('category')\ntrain_df['Vehicle_Damage_Yes'] = train_df['Vehicle_Damage_Yes'].astype('category')\ntrain_df['Gender_Male'] = train_df['Gender_Male'].astype('category')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:05.173541Z","iopub.execute_input":"2024-07-16T06:34:05.173888Z","iopub.status.idle":"2024-07-16T06:34:23.986578Z","shell.execute_reply.started":"2024-07-16T06:34:05.173858Z","shell.execute_reply":"2024-07-16T06:34:23.985549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:23.990979Z","iopub.execute_input":"2024-07-16T06:34:23.991358Z","iopub.status.idle":"2024-07-16T06:34:24.013965Z","shell.execute_reply.started":"2024-07-16T06:34:23.991327Z","shell.execute_reply":"2024-07-16T06:34:24.012666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4.2 Client Total Sales","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.5\"></a>\n### <b>3.5 <span style='color:#E1B12D'> Feature Selection </span></b> ","metadata":{}},{"cell_type":"code","source":"numeric_columns = [col for col in train_df.columns if col not in CFG.CATEGORICAL_VARIABLES]\ncorr_matrix = train_df.head(10000)[numeric_columns].corr()\n# Plot the heatmap\nplt.figure(figsize=(20, 16))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix with Heatmap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:24.015575Z","iopub.execute_input":"2024-07-16T06:34:24.016089Z","iopub.status.idle":"2024-07-16T06:34:24.778782Z","shell.execute_reply.started":"2024-07-16T06:34:24.016044Z","shell.execute_reply":"2024-07-16T06:34:24.777689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def clean_column_names(columns):\n#     \"\"\"Clean column names by replacing problematic characters.\"\"\"\n#     return [col.replace('[', '').replace(']', '').replace('<', '').replace('>', '') for col in columns]\n\n# def feature_selection(df, CFG, number_of_features):\n#     \"\"\"Perform feature selection using XGBoost feature importances for classification\"\"\"\n#     sample_df = df.sample(frac=0.05, random_state=CFG.SEED)\n#     X = sample_df.drop(columns=CFG.TARGET_VARIABLE)\n#     y = sample_df[CFG.TARGET_VARIABLE]\n\n#     # Clean column names\n#     X.columns = clean_column_names(X.columns)\n\n#     model = xgb.XGBClassifier(n_estimators=100, random_state=CFG.SEED, tree_method='hist')\n\n#     # Fit the model\n#     model.fit(X, y)\n#     importances = model.feature_importances_\n#     indices = np.argsort(importances)[::-1]\n\n#     # Print the feature ranking\n#     print(\"Feature ranking:\")\n#     feature_ranking = [(X.columns[indices[f]], importances[indices[f]]) for f in range(X.shape[1])]\n#     for rank, (feature, importance) in enumerate(feature_ranking, start=1):\n#         print(f\"{rank}. feature {feature} ({importance})\")\n\n#     # Get the top features\n#     selected_features = X.columns[indices[:number_of_features]]  # Adjust the number of features as needed\n#     print(\"Selected Features by XGBoost:\", selected_features)\n\n#     return selected_features\n\n# # Assuming train_df and CFG are defined as before\n# selected_features = feature_selection(train_df, CFG, 16)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:24.780269Z","iopub.execute_input":"2024-07-16T06:34:24.780665Z","iopub.status.idle":"2024-07-16T06:34:24.788563Z","shell.execute_reply.started":"2024-07-16T06:34:24.780630Z","shell.execute_reply":"2024-07-16T06:34:24.787356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def selected_features_dataframe(df,selected_features):\n#     \"\"\"Select the important features from dataframe\"\"\"\n#     selected_features = list(selected_features)\n#     selected_features.append(CFG.TARGET_VARIABLE)\n#     df=df[selected_features]\n#     return df \n# train_df = selected_features_dataframe(train_df,selected_features)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:24.789922Z","iopub.execute_input":"2024-07-16T06:34:24.790299Z","iopub.status.idle":"2024-07-16T06:34:24.800447Z","shell.execute_reply.started":"2024-07-16T06:34:24.790258Z","shell.execute_reply":"2024-07-16T06:34:24.799389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #E1B12D; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n<a id=\"4\"></a>\n## <b>4 <span style='color:#E1B12D'> Data Pipeline </span></b> \n### <b>4.1 <span style='color:#E1B12D'> Pipeline Creation </span></b> ","metadata":{}},{"cell_type":"code","source":"def preprocess_train_data(train_df, numerical_columns, categorical_columns):\n    # Map Vehicle Age\n    train_df = map_vehicle_age(train_df)\n\n    # Standard Scaling\n    train_df = standard_scale_train(train_df, numerical_columns)\n    # One-Hot Encoding\n    train_df = one_hot_encode_train(train_df, categorical_columns)\n    # Add Interaction Features\n    train_df = add_interaction_features(train_df, 'Age', 'Annual_Premium')\n    train_df = add_interaction_features(train_df, 'Age', 'Vintage')\n    return train_df","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:24.801766Z","iopub.execute_input":"2024-07-16T06:34:24.802104Z","iopub.status.idle":"2024-07-16T06:34:24.816492Z","shell.execute_reply.started":"2024-07-16T06:34:24.802074Z","shell.execute_reply":"2024-07-16T06:34:24.815419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test_data(test_df, numerical_columns, categorical_columns):\n    # Map Vehicle Age\n    test_df = map_vehicle_age(test_df)\n\n    # Load and apply Standard Scaler\n    scaler = joblib.load('scaler.pkl')\n    test_df[numerical_columns] = scaler.transform(test_df[numerical_columns])\n\n    # Load and apply One-Hot Encoder\n    encoder = joblib.load('encoder.pkl')\n    encoded_df = encoder.transform(test_df[categorical_columns])\n    encoded_df = pd.DataFrame(encoded_df, columns=encoder.get_feature_names_out(categorical_columns))\n    test_df = test_df.drop(columns=categorical_columns).reset_index(drop=True)\n    test_df = pd.concat([test_df, encoded_df], axis=1)\n\n    # Add Interaction Features\n    test_df = add_interaction_features(test_df, 'Age', 'Annual_Premium')\n    test_df = add_interaction_features(test_df, 'Age', 'Vintage')\n    test_df['Region_Code'] = test_df['Region_Code'].astype('int')\n    test_df['Region_Code'] = test_df['Region_Code'].astype('category')\n    test_df['Vehicle_Age'] = test_df['Vehicle_Age'].astype('category')\n    return test_df","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:24.818187Z","iopub.execute_input":"2024-07-16T06:34:24.818533Z","iopub.status.idle":"2024-07-16T06:34:24.830761Z","shell.execute_reply.started":"2024-07-16T06:34:24.818503Z","shell.execute_reply":"2024-07-16T06:34:24.829577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n### <b>4.2 <span style='color:#E1B12D'> Visualizing the Pipeline </span></b> ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #E1B12D; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n<a id=\"5\"></a>\n## <b>5 <span style='color:#E1B12D'> Model Building </span></b> \n### <b>5.1 <span style='color:#E1B12D'> Train Test Split </span></b> ","metadata":{}},{"cell_type":"code","source":"def prepare_train_test_data(df, target_column, test_size=0.05, random_state=42):\n    \"\"\"Splits the data into training and testing sets.\"\"\"\n    X = df.drop(columns=target_column)\n    y = df[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n    return X_train, y_train, X_test, y_test","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:24.832255Z","iopub.execute_input":"2024-07-16T06:34:24.832669Z","iopub.status.idle":"2024-07-16T06:34:24.842342Z","shell.execute_reply.started":"2024-07-16T06:34:24.832639Z","shell.execute_reply":"2024-07-16T06:34:24.841158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train, X_test, y_test = prepare_train_test_data(train_df, target_column='Response')\ndel train_df","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:24.843823Z","iopub.execute_input":"2024-07-16T06:34:24.844613Z","iopub.status.idle":"2024-07-16T06:34:32.723968Z","shell.execute_reply.started":"2024-07-16T06:34:24.844572Z","shell.execute_reply":"2024-07-16T06:34:32.722836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:32.725180Z","iopub.execute_input":"2024-07-16T06:34:32.725509Z","iopub.status.idle":"2024-07-16T06:34:33.162475Z","shell.execute_reply.started":"2024-07-16T06:34:32.725479Z","shell.execute_reply":"2024-07-16T06:34:33.161369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n### <b>5.2 <span style='color:#E1B12D'> Training the Model </span></b> ","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate(model_config, X_train, y_train, experiment_id, save_best_model=True, save_dir=CFG.MODEL_SAVE_PATH, categorical_features=CFG.CATEGORICAL_VARIABLES):\n    \"\"\"\n    Train and evaluate a model using time series cross-validation for classification.\n    \"\"\"\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=CFG.SEED)\n    cv_scores = []\n    best_score = float('-inf')\n    best_model = None\n    model_type = model_config['model'].__name__\n    model = model_config['model'](**model_config['params'])\n\n    with mlflow.start_run(experiment_id=experiment_id):\n        mlflow.log_param(\"model_name\", model_type)\n        for train_idx, val_idx in skf.split(X_train, y_train):\n            X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n            # Clone and train the model\n            fold_model = clone(model)\n            if model_type == 'LGBMClassifier':\n                fold_model.fit(X_train_fold, y_train_fold)\n            elif model_type == 'XGBClassifier':\n                fold_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], verbose=False)\n            elif model_type == 'CatBoostClassifier':\n                fold_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], cat_features=categorical_features, early_stopping_rounds=10, verbose=False)\n            else:\n                fold_model.fit(X_train_fold, y_train_fold)\n\n            # Evaluate the model\n            y_val_pred = fold_model.predict_proba(X_val_fold)[:, 1]\n            val_roc_auc = roc_auc_score(y_val_fold, y_val_pred)\n            cv_scores.append(val_roc_auc)\n            mlflow.log_metric(f\"val_roc_auc_fold_{len(cv_scores)}\", val_roc_auc)\n\n            # Check if the current fold's model is the best one for this algorithm\n            if val_roc_auc > best_score:\n                best_score = val_roc_auc\n                best_model = fold_model\n\n        mlflow.log_params(model_config['params'])\n        mlflow.log_metric(\"best_val_roc_auc\", best_score)\n\n    # Save the best model for this algorithm\n    if save_best_model:\n        best_model_filename = f\"{model_type}_best_model.joblib\"\n        best_model_save = os.path.join(save_dir, experiment_id)\n        check_and_create_directory(best_model_save)\n        joblib.dump(best_model, os.path.join(best_model_save, best_model_filename))\n        print(f\"Best model ({model_type}) with ROC AUC {best_score:.4f} saved to {best_model_filename}\")\n\n    return np.mean(cv_scores)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:34:33.164290Z","iopub.execute_input":"2024-07-16T06:34:33.164638Z","iopub.status.idle":"2024-07-16T06:34:33.180940Z","shell.execute_reply.started":"2024-07-16T06:34:33.164596Z","shell.execute_reply":"2024-07-16T06:34:33.179429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiating for Experiment tracking\ncheck_and_create_directory(CFG.MODEL_SAVE_PATH) # to create directory to save models\ncheck_and_create_directory(CFG.MLFLOW_LOCAL_URI) # to create directory for experiment tracking\nif CFG.EXPERIMENT_RECORDING =='LOCAL':\n    mlflow.set_tracking_uri(CFG.MLFLOW_LOCAL_URI)\nelse:\n    dagshub.init(CFG.DAGSHUB_REPO, CFG.DAGSHUB_REPO_OWNER, mlflow=True)\nexperiment_id = get_or_create_experiment_id(CFG.MLFLOW_EXPERIMENT_NAME)\nfor model_name, model_config in CFG.MODELS.items():\n    print(f\"Training and evaluating model: {model_name}\")\n    average_rmse = train_and_evaluate(model_config, X_train, y_train,experiment_id)\n    print(f\"ROC AUC for {model_name}: {average_rmse:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:38:10.439956Z","iopub.execute_input":"2024-07-16T06:38:10.440783Z","iopub.status.idle":"2024-07-16T06:41:35.741658Z","shell.execute_reply.started":"2024-07-16T06:38:10.440746Z","shell.execute_reply":"2024-07-16T06:41:35.740663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.3\"></a>\n### <b>5.3 <span style='color:#E1B12D'>  Performance Evaluation </span></b> ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve, auc\ndef load_and_evaluate_models(model_directory, X_test, y_test, experiment_id):\n    \"\"\"Evaluate the Performance of Various Models using Test Data\"\"\"\n    # List all files in the model directory\n    print(os.listdir(os.path.join(model_directory, experiment_id)))\n    model_files = [f for f in os.listdir(os.path.join(model_directory, experiment_id)) if f.endswith('.joblib')]\n\n    results = []\n\n    # Iterate over the found model files\n    for model_file in model_files:\n        # Load the model\n        model_path = os.path.join(model_directory, experiment_id, model_file)\n        model = joblib.load(model_path)\n        print(f\"Loaded model from {model_path}\")\n\n        # Predict on the test set\n        y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n\n        # Evaluate the model on the test set\n        test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n        print(f'Test ROC AUC: {test_roc_auc}')\n\n        # Store the results in the list\n        results.append({\n            'Model': model_file,\n            'ROC AUC': test_roc_auc\n        })\n\n        # Plot ROC curve\n        fpr, tpr, _ = roc_curve(y_test, y_test_pred_proba)\n        roc_auc = auc(fpr, tpr)\n        plt.figure(figsize=(10, 6))\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'Receiver Operating Characteristic - {model_file}')\n        plt.legend(loc='lower right')\n        plt.show()\n\n    del model\n\n    # Convert the list of results to a DataFrame\n    results_df = pd.DataFrame(results)\n\n    return results_df\n\n# Assuming model_directory, X_test, y_test, and experiment_id are defined\nmodel_directory = CFG.MODEL_SAVE_PATH\nresults = load_and_evaluate_models(model_directory, X_test, y_test, experiment_id)\n\n\n# Display the evaluation results\nprint(results)\n\n# Cleanup after execution\ndel X_train\ndel y_train\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:42:00.568975Z","iopub.execute_input":"2024-07-16T06:42:00.569658Z","iopub.status.idle":"2024-07-16T06:42:01.783450Z","shell.execute_reply.started":"2024-07-16T06:42:00.569613Z","shell.execute_reply":"2024-07-16T06:42:01.782468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.4\"></a>\n### <b>5.4 <span style='color:#E1B12D'> Ensembling for Performance Improvement </span></b> ","metadata":{}},{"cell_type":"code","source":"def load_models(model_directory):\n    \"\"\"Load all models from the specified directory.\"\"\"\n    models = {}\n    for model_name in ['XGBClassifier_best', 'CatBoostClassifier_best', 'LGBMClassifier_best']:\n        file_path = os.path.join(model_directory, f\"{model_name}_model.joblib\")\n        models[model_name] = joblib.load(file_path)\n    return models\n\n# Directory where models are saved\nmodel_directory = '/kaggle/working/models/' + experiment_id\n\n# Load models\nmodels = load_models(model_directory)\n\ndef objective(trial):\n    \"\"\"Enhance the performance of ensembling and optimizing weights.\"\"\"\n    # Suggest weights for each model\n    w_catboost = trial.suggest_float('w_catboost', 0, 1)\n    w_xgboost = trial.suggest_float('w_xgboost', 0, 1)\n    w_lightgbm = trial.suggest_float('w_lightgbm', 0, 1)\n    \n    # Normalize weights so they sum to 1\n    total = w_catboost + w_xgboost + w_lightgbm\n    w_catboost /= total\n    w_xgboost /= total\n    w_lightgbm /= total\n    \n    # Collect individual model predictions\n    y_pred_cat = models['CatBoostClassifier_best'].predict_proba(X_test)[:, 1]\n    y_pred_xgb = models['XGBClassifier_best'].predict_proba(X_test)[:, 1]\n    y_pred_lgb = models['LGBMClassifier_best'].predict_proba(X_test)[:, 1]\n\n    # Combine predictions based on the weights\n    y_pred = (w_catboost * y_pred_cat + w_xgboost * y_pred_xgb + w_lightgbm * y_pred_lgb)\n\n    # Calculate and return the ROC AUC score\n    roc_auc = roc_auc_score(y_test, y_pred)\n    return roc_auc\n\n# Set up Optuna study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n\n# Retrieve the best weights\nbest_weights = {\n    'CatBoostClassifier_best': study.best_trial.params['w_catboost'],\n    'XGBClassifier_best': study.best_trial.params['w_xgboost'],\n    'LGBMClassifier_best': study.best_trial.params['w_lightgbm']\n}\ntotal = sum(best_weights.values())\nbest_weights = {model: weight / total for model, weight in best_weights.items()}\nprint(\"Best weights:\", best_weights)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.591176Z","iopub.status.idle":"2024-07-16T06:14:55.591690Z","shell.execute_reply.started":"2024-07-16T06:14:55.591432Z","shell.execute_reply":"2024-07-16T06:14:55.591454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_ensemble(X_test, model_directory, best_weights):\n    \"\"\"Make predictions using an ensemble of pre-trained models and specified weights.\"\"\"\n    models = load_models(model_directory)\n    # Collect individual model predictions\n    y_pred_cat = models['CatBoostClassifier_best'].predict_proba(X_test)[:, 1]\n    y_pred_xgb = models['XGBClassifier_best'].predict_proba(X_test)[:, 1]\n    y_pred_lgb = models['LGBMClassifier_best'].predict_proba(X_test)[:, 1]\n\n    # Combine predictions based on the weights\n    y_pred = (best_weights['CatBoostClassifier_best'] * y_pred_cat +\n              best_weights['XGBClassifier_best'] * y_pred_xgb +\n              best_weights['LGBMClassifier_best'] * y_pred_lgb)\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.593602Z","iopub.status.idle":"2024-07-16T06:14:55.593975Z","shell.execute_reply.started":"2024-07-16T06:14:55.593781Z","shell.execute_reply":"2024-07-16T06:14:55.593808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.5\"></a>\n### <b>5.5 <span style='color:#E1B12D'> Model Explainablity </span></b> ","metadata":{}},{"cell_type":"code","source":"# def model_explainability(model_directory, X_test):\n#     \"\"\"Provide the importance of Features in the Model\"\"\"\n#     shap_values_dict = {}\n#     X_test_sampled = X_test.sample(frac=0.005)  # Ensure variable naming consistency\n#     models = load_models(model_directory)\n#     for name, model in models.items():\n#         print(f\"Explaining model: {name}\")\n#         if 'catboost' in name.lower():\n#             explainer = shap.TreeExplainer(model, feature_perturbation='interventional', check_additivity=False)\n#             shap_values = explainer(X_test_sampled, check_additivity=False)\n#         elif 'xgb' in name.lower() or 'lgbm' in name.lower():\n#             explainer = shap.TreeExplainer(model, check_additivity=False)\n#             shap_values = explainer(X_test_sampled, check_additivity=False)\n#         else:\n#             explainer = shap.Explainer(model)\n#             shap_values = explainer.shap_values(X_test_sampled)\n#         if isinstance(shap_values, list):\n#             shap_values = shap_values[1]  # For binary classification, use the positive class SHAP values\n        \n#         # Ensure feature names are passed correctly\n        \n#         shap_values_dict[name] = shap_values\n        \n#         # Plot summary plot for the current model\n#         shap.summary_plot(shap_values, X_test_sampled, show=False)\n#         plt.title(f\"SHAP Summary Plot for {name}\")\n#         plt.show()\n        \n#         # Plot feature importance bar chart for the current model\n#         shap.plots.bar(shap_values, show=False)\n#         plt.title(f\"SHAP Feature Importance for {name}\")\n#         plt.show()\n    \n#     return shap_values_dict\n\n# # Example usage\n# shap_values_dict = model_explainability(model_directory, X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.594974Z","iopub.status.idle":"2024-07-16T06:14:55.595370Z","shell.execute_reply.started":"2024-07-16T06:14:55.595183Z","shell.execute_reply":"2024-07-16T06:14:55.595200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #E1B12D; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n<a id=\"6\"></a>\n## <b>6 <span style='color:#E1B12D'> Model Inference </span></b> \n<a id=\"6.1\"></a>\n### <b>6.1 <span style='color:#E1B12D'> Loading the Model </span></b> ","metadata":{}},{"cell_type":"code","source":"def predict_ensemble(X_test, model_directory, best_weights):\n    \"\"\"Make predictions using an ensemble of pre-trained models and specified weights.\"\"\"\n    models = load_models(model_directory)\n    # Collect individual model predictions\n    y_pred_cat = models['CatBoostClassifier_best'].predict_proba(X_test)[:, 1]\n    y_pred_xgb = models['XGBClassifier_best'].predict_proba(X_test)[:, 1]\n    y_pred_lgb = models['LGBMClassifier_best'].predict_proba(X_test)[:, 1]\n\n    # Combine predictions based on the weights\n    y_pred = (best_weights['CatBoostClassifier_best'] * y_pred_cat +\n              best_weights['XGBClassifier_best'] * y_pred_xgb +\n              best_weights['LGBMClassifier_best'] * y_pred_lgb)\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.597423Z","iopub.status.idle":"2024-07-16T06:14:55.597952Z","shell.execute_reply.started":"2024-07-16T06:14:55.597678Z","shell.execute_reply":"2024-07-16T06:14:55.597700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2\"></a>\n### <b>6.2 <span style='color:#E1B12D'> Data Loading and Prepration </span></b> \n#### 6.2.1 Reading the data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(CFG.TEST_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.599521Z","iopub.status.idle":"2024-07-16T06:14:55.600040Z","shell.execute_reply.started":"2024-07-16T06:14:55.599763Z","shell.execute_reply":"2024-07-16T06:14:55.599785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.2.2 Processing the Data","metadata":{}},{"cell_type":"code","source":"test_df_preprocessed = preprocess_test_data(test_df, numerical_cols, categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.602212Z","iopub.status.idle":"2024-07-16T06:14:55.602596Z","shell.execute_reply.started":"2024-07-16T06:14:55.602401Z","shell.execute_reply":"2024-07-16T06:14:55.602417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_preprocessed['Vehicle_Age']=test_df_preprocessed['Vehicle_Age'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.603770Z","iopub.status.idle":"2024-07-16T06:14:55.604188Z","shell.execute_reply.started":"2024-07-16T06:14:55.603971Z","shell.execute_reply":"2024-07-16T06:14:55.603989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.3\"></a>\n### <b>6.3 <span style='color:#E1B12D'> Model Forecasting </span></b> ","metadata":{}},{"cell_type":"code","source":"def prediction(df):\n    y_pred = predict_ensemble(df, model_directory, best_weights)\n    df['Response'] = y_pred\n    df = df[['id','Response']]\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.605803Z","iopub.status.idle":"2024-07-16T06:14:55.606229Z","shell.execute_reply.started":"2024-07-16T06:14:55.606005Z","shell.execute_reply":"2024-07-16T06:14:55.606022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = prediction(test_df_preprocessed)\ntest_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.607672Z","iopub.status.idle":"2024-07-16T06:14:55.608031Z","shell.execute_reply.started":"2024-07-16T06:14:55.607854Z","shell.execute_reply":"2024-07-16T06:14:55.607870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mlflow Local UI","metadata":{}},{"cell_type":"code","source":"# if CFG.EXPERIMENT_RECORDING == 'LOCAL':\n#     !pip install mlflow --quiet\n#     !pip install pyngrok --quiet\n    \n#     from pyngrok import ngrok\n#     from getpass import getpass\n\n#     ngrok.kill()\n\n#     # Setting the authtoken (replace with your own token from https://dashboard.ngrok.com/auth)\n#     NGROK_AUTH_TOKEN = \"2ioyKWLGBeFWH5D7vI5ktAJaZSV_2aUaXGdGJeHsk43qWXRvk\"\n#     ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\n#     # Start the MLflow UI\n#     get_ipython().system_raw(\"mlflow ui --backend-store-uri file:/kaggle/working/mlflow --host 0.0.0.0 --port 5000 &\")\n\n#     # Open an HTTPs tunnel on port 5000 for http://localhost:5000\n#     ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n#     print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:14:55.609178Z","iopub.status.idle":"2024-07-16T06:14:55.609551Z","shell.execute_reply.started":"2024-07-16T06:14:55.609367Z","shell.execute_reply":"2024-07-16T06:14:55.609383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}